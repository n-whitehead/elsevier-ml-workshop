{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models & Support Vector Machines\n",
    "\n",
    "In this lesson we will explore a new way to think about classification. Rather than trying to directly classify new examples based on old ones to model the entire distribution of classes (eg. *k*-Nearest Neighbor), we can simply model the *boundaries* between different classes.\n",
    "\n",
    "First, we will gain some intuition around what modeling boundaries means, and define some terms to help guide our motivation before exploring two different algorithms for boundary creation.\n",
    "\n",
    "Next, we will explore the first of our boundary creation algorithms: Linear Discriminant Analysis (LDA). LDA will be our first exposure to representation learning; specifically classification via dimensionality reduction. LDA is a supervised algorithm, typically used for binary (two class) classification problems.\n",
    "\n",
    "Finally, we will expand on LDA's data transformation techniques with Support Vector Machines. SVM is a supervised algorithm, typically used for classification or regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries\n",
    "\n",
    "We are going to shift our way of thinking about classification from directly classifying new examples based on old ones, to directly modeling a function to separate classes (ie. creating boundaries). The **discriminant** is the function that models decision boundaries between classes. In a well-bounded space, a novel data point will simply belong to the class it is bounded by. This bounded region is known as the **decision region**.\n",
    "\n",
    "Like all frameworks we choose, there are trade-offs we must consider. In the case of classification, creating decision region between classes depends on how complex the discriminant is. If the discriminant is simple (compared to the class data point distribution) then this is a good approach. However, if the data presents ambiguous overlap between class distributions, then this method will not make accurate predictions. In theory, using very complex, non-linear discriminants to approximate boundaries sounds nice, and can potentially separate boundaries perfectly. In practice, however, it often results in a space that is intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/3_discriminant.png\" alt=\"http://uc-r.github.io/discriminant_analysis\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the image above with class distributions 1 (red), 2 (green), and 3 (blue). The plane the distribution exists in (not necessarily a 2-dimensional plane!) is separated into three distinct sections by two (or three) purple discriminant lines. Here is where the intuition behind our trade-off begin to take effect. The image on the left is divided by linear boundaries, which is performant for many classes but lacks predictive accuracy (notice the red \"1s\" creeping into the blue \"3s\" boundary). The image on the right will improve on the linear models accuracy by generating non-linear boundaries, quadratic in this case, at the expense of an irregular hypothesis space. The above example is simple enough that a non-linear boundary for three classes is feasible, if we consider examples with many dimensions and many classes, then generating non-linear function boundaries becomes much harder to manage.\n",
    "\n",
    "First, we will explore the simplest case, by restricting ourselves to *linear* discriminant functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Intuition\n",
    "\n",
    "Let us once again consider the mathematics behind our newly found intuition.\n",
    "\n",
    "When we say \"linear\" we mean the boundary will be a **hyperplane** (or **decision surface**), which is a space 1 dimension less than our example space. In 2-dimensions this hyperplane is a line, but in higher dimensions we can still refer to this as a \"linear\" model.\n",
    "\n",
    "In low dimensional spaces, the concept of a hyperplane is immediately obvious. However in much higher dimensions it becomes nearly impossible to conceptualize, let alone determine which boundary a data point exists in. Luckily, there is only one trick you need to know to determine which side of a hyperplane you are on. Consider the example vector $\\mathbf{a}=[a_1\\:\\dots\\:a_n]$, and the hyperplane boundary $\\mathbf{w}=[w_1\\:\\dots\\:w_n]$. Project the point onto a line perpendicular to the hyperplane using a **dot product** of vectors, written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{a}\\cdot\\mathbf{w}=\\sum_{i=1}^n{a_iw_i}=a_1w_1+\\dots+a_nw_n\n",
    "\\label{3.1}\\tag{3.1}\n",
    "\\end{equation*}\n",
    "\n",
    "Next, check whether the projected value (which is a scalar) is greater or less than a threshold value based on where the hyperplane intersects the line (ie. $\\mathbf{a}\\cdot\\mathbf{w}+b<0$, and $\\mathbf{a}\\cdot\\mathbf{w}+b>0$, where $b$ is a scalar threshold value.\n",
    "\n",
    "If our data point can be correctly classified by this hyperplane boundary, then we say the classes are **linearly separable**. If no hyperplane exists that can separate our classes correctly, we say our data is not linearly separable.\n",
    "\n",
    "How do we choose how many boundaries exist for our dataset? Well, it is trivial to show a single boundary can separate two classes of data points. If we are presented with more than two classes we need multiple decision boundaries. The typical approach is to use a *one-vs-all* method, meaning there will be one decision boundary per class. This can lead to some problems, specifically where a new point lies in a region intersected by more than one hyperplane. There are several algorithms that exist to choose a hyperplane, most notably is through \"gradient descent\", which works by picking a random candidate split plane, and repeatedly making small adjustments to it in ways that reduce the error measure. While this level of specificity is beyond the scope of this lesson, if this subject is interesting to you I encourage you to read about techniques for generating \"good\" hyperplane boundaries.\n",
    "\n",
    "As we will see, finding the \"best\" boundary hyperplane is non-trivial. There may be choices that give perfect accuracy on a training set, or there may be none. In either case, let's first explore a linear technique for generating boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "Consider following data point distribution of two classes, red and blue:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/3_linearly_inseperable_data.png\" alt=\"https://sthalles.github.io/fisher-linear-discriminant/\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there is no way to draw a line to separate the red and blue points and achieve good predictive results, as seen in the left hand image below. Instead, we can *transform* the data in such a way that we can separate classes with a line, as seen in the right hand image below by squaring the input feature vectors. How did we know to square the input feature vectors to make this problem linearly separable? The answer to this is not trivial, and involves learning a representation, which we will cover in more depth in the \"Deep Learning\" lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/3_feature_transformation.png\" alt=\"https://sthalles.github.io/fisher-linear-discriminant/\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA, often referred to as **Fisherâ€™s Linear Discriminant**, relies on a process of transforming data into a lower dimensional representation of the original space, known as **dimensionality reduction**. Dimensionality reduction will be covered in much greater detail in Lesson 8, but for now we will use it as a tool to create boundaries in two-class datasets. We attempt to find a transformation representation of our data samples by projecting the samples onto a plane of one less dimension. You can essentially run the LDA repeatedly as a form of dimensionality reduction (as long as the projection is orthogonal to the last), but this is not advised for classification problems. Typically, you want to project your data into a space one dimension less, then apply some other classification algorithm to the projected data.\n",
    "\n",
    "Fisher wanted to maximize a function that returns a large variance among classes, and a small variance within classes. In other words, a function that, when maximized, increases the distance between *different* classes, and reduces the distance within *same* class clusters. Recall our issue stated in the Introduction, where a linear separation is not accurate, since there are classes of a different label belonging to another label's decision region. LDA attempts to resolve this issue by finding a *distinct* linear boundary via changing the representation of the original data. \n",
    "\n",
    "It should be noted, it is possible that LDA won't be able to find a linearly separable boundary between classes that gives reasonable accuracy. We are simply gaining another method to make predictions; and in Lesson 6 we will apply our machine learning models to different types of data, then evaluate and understand why some models work better in different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "\n",
    "For the sake of example, we will consider a problem space in $\\mathbb{R}^2$, and a projection space in $\\mathbb{R}$ represented by a straight line. Consider the image below, where classes (represented as green and black dots on an xy-axis) are *projected* onto the hyperplane in red.\n",
    "\n",
    "Once points are projected onto a line, we can describe how they are dispersed. In the image below, if we were to split the red line in half we would notice the majority of green points exist above our imaginary split, and the majority of black points exist below it. We would like to find a distribution of points that is cleanly separated such that when a new point is added we can easily classify it based on what side of the hyperplane it exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/3_lda.jpg\" alt=\"https://www.geeksforgeeks.org/ml-linear-discriminant-analysis/\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does LDA help us choose a \"best\" hyperplane configuration? Fisher outlines the following two criteria:\n",
    "1. Maximizing the distance between mean projected values of different classes.\n",
    "2. Minimizing the **scatter**, or the \"same-class\" variance in the projected space, within each class.\n",
    "\n",
    "Formally, LDA aims to maximize the **objective function** $J$, the ratio of the *absolute* difference of per-class projected means ($\\hat{m}$) to the sum of per class scatters ($\\hat{s}$) for classes $A$ and $B$:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w})=\\frac{(\\hat{m_A}-\\hat{m_B})^2}{\\hat{s_A}+\\hat{s_B}}\n",
    "\\label{3.2}\\tag{3.2}\n",
    "\\end{equation}\n",
    "\n",
    "Ideally, the numerator of \\ref{3.2} will be large, and the denominator will be small, satisfying the criteria of maximization above. Let's dig a little deeper into what LDA is actually doing.\n",
    "\n",
    "Let vector $\\mathbf{w}$ be a vector **normal**, or perpendicular, to the hyperplane, illustrated by the dotted lines above. $\\mathbf{w}$ is used to project vector $\\mathbf{x}$ from our input space in $\\mathbb{R}^2$ onto $y$, a scalar value in $\\mathbb{R}$. Clearly, the size of $\\mathbf{w}$ is the same as $\\mathbf{x}$, as both are row vectors. We can write our scalar value $y$ as a linear function:\n",
    "\n",
    "\\begin{equation}\n",
    "y=\\mathbf{w}^\\intercal\\mathbf{x}\n",
    "\\label{3.3}\\tag{3.3}\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\mathbf{w}^\\intercal$ is the **transpose** of vector $\\mathbf{w}$.\n",
    "\n",
    "Let $t$ be the threshold that separates classes in our projected space. For an input vector $\\mathbf{x}$, if the projected value $y\\geq t$ then $\\mathbf{x}$ belongs to class $A$, otherwise $\\mathbf{x}$ belongs to class $B$. We would like to maximize the separability of projected points $y$ based on our first criteria.\n",
    "\n",
    "In order to do this, we must first define a measure of separation between projected classes. Let $m_i$, and $\\hat{m_i}$ represent the mean scalar distance between each class, $C_i$. The number of 2-dimensional samples of $C_i$ is represented by the set $N_i$, hence we have a matrix of 2-dimensional vectors, \n",
    "$\\begin{Bmatrix} \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_{N_i} \\end{Bmatrix}$ belonging to a set $\\omega_i$ such that:\n",
    "\n",
    "\n",
    "RETURN TO THIS!!! FIND OLD ARTICLE\n",
    "\n",
    "\\begin{align}\n",
    "m_i&=\\frac{1}{N_i}\\sum_{\\mathbf{x}\\in\\omega_i}{\\mathbf{x}} \\\\\n",
    "\\hat{m_i}&=\\frac{1}{N_i}\\sum_{y\\in\\omega_i}{y}=\\frac{1}{N_i}\\sum_{\\mathbf{x}\\in\\omega_i}{\\mathbf{w}^\\intercal\\mathbf{x}}=\\frac{\\mathbf{w}^\\intercal}{N_i}\\sum_{\\mathbf{x}\\in\\omega_i}{\\mathbf{x}}=\\mathbf{w}^\\intercal m_i\n",
    "\\label{3.4}\\tag{3.4}\n",
    "\\end{align}\n",
    "\n",
    "We aren't quite done yet, as Fisher Linear Discriminant function also aims to minimize the same-class variance, or scatter, defined in our second criteria. Let $s_i$, and $\\hat{s_i}$ represent the same-class variance for class $i$ in the original and projected space respectively. The variance is the sum of square differences between values and their classes mean:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{s_i}&=\\sum_{y\\in i}{(y-\\hat{m_i})^2} \\\\\n",
    "s_i&=\\sum_{\\mathbf{x}\\in i}{(\\mathbf{w}^\\intercal\\mathbf{x}-\\mathbf{w}^\\intercal m_i)^2}=\\mathbf{w}^\\intercal\\left(\\sum_{\\mathbf{x}\\in i}{(x-m_i)(x-m_i)^\\intercal}\\right)\\mathbf{w}\n",
    "\\label{3.5}\\tag{3.5}\n",
    "\\end{align}\n",
    "\n",
    "Let $S_i$ represent the **covariance matrix**, or **scatter matrix** for class $i$:\n",
    "\\begin{equation*}\n",
    "S_i=\\left(\\sum_{\\vec{x}\\in C_i}{(x-m_i)(x-m_i)^\\intercal}\\right)\n",
    "\\tag{3.6}\n",
    "\\end{equation*}\n",
    "\n",
    "The same-class scatter matrix, $S_w$, for classes $a$, and $b$ is $S_w=S_a+S_b$. Therefore we can write equation (3.5) as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{s_a}+\\hat{s_b}=\\vec{w}^\\intercal S_w\\vec{w}=\\hat{S_w}\n",
    "\\tag{3.7}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $\\hat{S_w}$ is the same-class scatter matrix of projected values $y$.\n",
    "\n",
    "Using a similar method, we can express the difference of means between the projected values in $y$ space in terms of our original $\\vec{x}$ space.\n",
    "\n",
    "\\begin{equation*}\n",
    "(\\hat{m_a}-\\hat{m_b})^2=\\vec{w}^\\intercal(m_a-m_b)(m_a-m_b)^\\intercal\\vec{w}=\\vec{w}^\\intercal S_m\\vec{w}=\\hat{S_m}\n",
    "\\tag{3.8}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $\\hat{S_m}$ is the different-class scatter matrix of projected values $y$.\n",
    "\n",
    "We can now write $J$ in terms of our original feature space:\n",
    "\n",
    "\\begin{equation*}\n",
    "J(\\vec{w})=\\frac{(\\hat{m_a}-\\hat{m_b})^2}{\\hat{s_a}+\\hat{s_b}}=\\frac{\\vec{w}^\\intercal S_w\\vec{w}}{\\vec{w}^\\intercal S_m\\vec{w}}\n",
    "\\tag{3.9}\n",
    "\\end{equation*}\n",
    "\n",
    "In order to find the maximum of $J$, we can differentiate and equate it to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra\n",
    "\n",
    "Recall from the section above, we can encode a hyperplane as a vector, $\\vec{w}$. Let us treat this vector as a **basis vector**, and project our data points onto it. A basis vector is a vector that **spans** a **vector space**, defined as follows: Given a vector space $V=\\mathbb{R}^n$, and a vector $\\vec{v}=[v_1\\:\\dots\\:v_n]\\in V$, $\\vec{v}$ is a basis vector for $V$ if every element in $V$ can be written as a unique linear combination of elements of $\\vec{v}$. Given this, we can now encode each data point as a scalar value (ie. a distance from the origin along the vector).\n",
    "\n",
    "Here is some intuition behind the paragraph above. Let's use our previously defined vector space $V=\\mathbb{R}^2$, defined in the x-y coordinate plane, where points are written $(x,y)$. Consider a point $(1,2)$ on this plane. We can write this coordinate as the sum (or linear combination) of it's $x$ and $y$ components:\n",
    "\n",
    "\\begin{equation*}\n",
    "(1,2)=1\\cdot(1,0)+2\\cdot(0,1)\n",
    "\\end{equation*}\n",
    "\n",
    "Where $(1,0)$ and $(0,1)$ are unit vectors. The pair of unit vectors *spans* any vector in $V$, since any vector in $V$ can be decomposed as a matrix:\n",
    "\n",
    "\\begin{equation*}\n",
    "(a,b)=a\\cdot(1,0)+b\\cdot(0,1)=\n",
    "\\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Notice that the unit vectors are not the only basis vectors for $V$, we can choose $\\{(1,1),(0,1)\\}$ and form the linear combination:\n",
    "\n",
    "\\begin{equation*}\n",
    "(a,b)=a\\cdot(1,1)+(b-a)\\cdot(0,1)\n",
    "\\end{equation*}\n",
    "\n",
    "In fact, we can take any real value $t\\in\\mathbb{R}$ such that:\n",
    "\\begin{equation*}\n",
    "(a,b)=a\\cdot(1,t)+(b-a)\\cdot(0,1)=\n",
    "\\begin{pmatrix}\n",
    "1 & t\\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "This combination of vectors will span any vector in $V$.\n",
    "\n",
    "We can multiply vectors (or 1-dimensional matrices) together as follows:\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}a\\\\b\\\\c\\end{bmatrix}\\times\\begin{bmatrix}d & e & f\\end{bmatrix}=a\\cdot d + b\\cdot e + c\\cdot f\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the result is a scalar value, and that we can only multiply a row vector by a column vector, or vice versa, since their dimensions must be compatible. Suppose we are given two column vectors (ie. $1\\times n$ matrices) $A$ and $B$. If we want to perform matrix multiplication, we must take the **transpose** of a matrix, say matrix $B$, denoted $B^\\intercal$, to convert it to a compatible row vector for multiplication with $A$, hence $A\\times B^\\intercal$.\n",
    "\n",
    "This is the extent of understanding required to understand LDA, but I encourage you to engage in your understanding of Linear Algebra as it will be useful for future lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
