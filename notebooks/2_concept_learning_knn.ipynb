{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Learning & *k*-Nearest Neighbor\n",
    "\n",
    "First, we will explore the idea of concept learning, which will be the foundation for conceptually understanding how we think about machine learning problems.\n",
    "\n",
    "Next, we will extract a popular toy dataset from a machine learning toolkit, SciKit Learn, and use Python to explore the structure of a dataset.\n",
    "\n",
    "Finally, we will introduce and apply our first elementary machine learning algorithm, *k*-Nearest Neighbor (kNN), to make simple predictions on an extracted dataset. While not practical in all but simple applications, understanding how kNN works will give us an intuition behind how concepts are learned, and how novel inputs are classified. kNN is a supervised algorithm, typically used for classification or regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Learning\n",
    "\n",
    "To **learn** a concept is to come up with a way of describing a general rule from a set of known examples. Before we can understand what it means to learn a concept, we need to make a few definitons, specifically what a concept and class is.\n",
    "\n",
    "A **concept** describes or defines a thing, or type of thing. For example, if we can understand the concept of a bicycle (number of wheels, mass, height, top speed, etc...), and we are given a bunch of other objects, like bicycles, cars, and houses, then the concept of a bicycle is sufficient to let us determine which objects are bicycles, and which are not. What type or amount of information is needed to fully specify a concept will depend on the problem domain, and the representation of information for that domain.\n",
    "\n",
    "A **class** is a group defined by a concept. Classes are identified by a label unique to the concept the class matches. We assign the label \"bicycle\" to all things in the bicycle class, where the bicycle class is made up of all the objects in the domain that match the bicycle concept.\n",
    "\n",
    "A **classification algorithm** takes concrete examples, and provides a class label for each example. Ideally, the class label is associated with the concept the label belongs to. If so, this is a true classification, otherwise it is false.\n",
    "\n",
    "A **classifier** is a model that performs a classification based on an algorithm definition. Given an example input, we expect the classifier to return a label with some degree of accuracy, and within a reasonable amount of time.\n",
    "\n",
    "Finally, we can define **concept learning** as a task, given a set of examples of which we know the labels, that generates a classifier to label any example (not just the ones we've already seen). In the next subsections we will talk about different types of learning that extend classifying labeled datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "Supervised learning is the process of learning from a sufficiently large set of examples with associated classes. If we have pre-labeled data to use, then this is the ideal form of learning, because we need some kind of 'supervisor' to tell us the correct label for each example we use for learning.\n",
    "\n",
    "#### Classification\n",
    "\n",
    "This is concept learning from our definition above. Eﬀectively speaking, so long as the expected output classes are discrete (ie. it is an element of a ﬁnite set), we can treat it as a classiﬁcation problem, regardless of how the discrete values are interpreted after we’re done.\n",
    "\n",
    "#### Regression\n",
    "\n",
    "This is function approximation. We use this form of learning when tasks require a numeric output of real-valued numbers. This can be defined as a single numeric value, or as a **vector**, which is a multi-variate numeric output. Estimation, or predictive modeling typically fall into this category.\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "If we have a lot of examples but no labels for them, we can use unsupervised learning. The goal of unsupervised learning is to 'invent' a useful set of classes that fit the data well, often called **clustering**. We try to find well-separated, compact clusters of data, and hypthesize that these clusters represent different classes. We could also attempt to build a model of the underlying distribution the data is sampled from, or perhaps discover a structure to the data that would be useful.\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "Reinforcement learning is generally useful for problems where we don't know what the correct action to take from a given state is. For example, looking at the state of an array of sensor values to determine what a robot should do next is difficult, but we can give a *reward* signal saying whether or not the given state is desirable to be in or not (ie. if the robot crashes it is in a bad state, but if it brings us a beer it will be rewarded, and in a good state). Over time, the model can use reward signals to learn how to make good decisions in an environment. Any discrete set of of actions a model can choose to make is synonymous with a standard classification problem.\n",
    "\n",
    "[Here](https://www.youtube.com/watch?v=Lt-KLtkDlh8) is an interesting demonstration of reinforcement learning. Over many iterations of receiving positive reward signals, the robot is able to account for disturbances in it's environment and self-correct.\n",
    "\n",
    "### Minimally-Supervised Learning\n",
    "\n",
    "Minimally-supervised learning is a hybrid learning technique where we only have a small set of labeled examples, and a larger set of unlabeled examples. In this case, we use the small number of examples we have to attempt to label some of the unlabeled examples, add them to the set of labeled examples, and repeat. We repeat this process until we have enough samples to perform supervised learning. The advantage of this technique is we don't have to label all of the examples by hand; however, the disadvantage is the accuracy of the labels is generally lower than if we hand labeled each example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first few lessons we will exclusively work with supervised learning algorithms, as they are the easiest to grasp, and will help us gain intuition behind why other techniques are required.\n",
    "\n",
    "There are many standard model frameworks for performing supervised classification or regression, each with their own pros and cons. Here is a list of commonly used frameworks we will cover:\n",
    "- *k*-nearest neighbor (kNN)\n",
    "- support vector machine (SVM)\n",
    "- decision tree\n",
    "- Na&iuml;ve Bayes\n",
    "- artificial neural networks\n",
    "\n",
    "We will find in our exploration of machine learning that there is no *one-size-fits-all* solution, and the choice of model depends entirely on the dataset and motivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Intuition\n",
    "\n",
    "At the end of each major section, I will reinforce our understanding with formal mathematical definitions of the broader concepts discussed above. I encourage you to read these sections and try to grasp the notation as it will provide a more complete picture of how the algorithms work. Since the scope of these lessons are not to teach mathematics, I will assume the reader has a rudimentary understanding of set notations, linear algebra, and probability. Before engaging in the 'Mathematical Intuition' sections, I highly recommend brushing up on basic concepts of each, or to review the concepts when I introduce them.\n",
    "\n",
    "Let us begin by introducing the notion of a **feature vector**, which is effectively a $n\\times 1$ row vector used to describe a multi-variate singular example from our dataset. In future lessons, feature vectors, or *vectors*, will appear in boldface, and typically contain a set of values:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}=\\begin{bmatrix} x_1 & x_2 & \\cdots & x_n \\end{bmatrix}\n",
    "\\label{eq:vector} \\tag{2.1}\n",
    "\\end{equation}\n",
    "\n",
    "The set of values in a feature vector are called **features**, or **dimensions**, and represent the attributes (ie. mass, volume, height, etc...) that describe the concept. Features are represented by $1\\times n$ column vectors. We say the vector in \\eqref{eq:vector} describes \"a feature vector, $\\mathbf{x}$, of $n$ dimensions\". Strictly speaking, a feature vector, $\\mathbf{x}\\in\\mathbb{R}^n$ (read as: a vector $\\mathbf{x}$ *belongs* to the set of real $n$-dimensional vectors, $\\mathbb{R}^n$), is a real valued vector in the $n$-dimensional (or $n$-feature) space, where $n$ corresponds to the number of features in our example. In a typical dataset we will have many feature vectors, as there are many examples, or 'rows', of data. We will refer to the set of $k$ feature vectors as:\n",
    "\n",
    "\\begin{equation}\n",
    "X=\\begin{Bmatrix} \\mathbf{x}_1 & \\cdots & \\mathbf{x}_k \\end{Bmatrix}\n",
    "\\label{eq:feature_vec_set} \\tag{2.2}\n",
    "\\end{equation}\n",
    "\n",
    "Hopefully $k$ is sufficiently large, and examples $\\mathbf{x}\\in X$ are diverse enough for a model to generalize a rule. For classification problems, we define the set of labels, or classes, as $y$. Thus, each example $\\mathbf{j}\\in X$ has a corresponding label $y^{(\\mathbf{j})}$.\n",
    "\n",
    "Consider our bicycle concept from earlier in this lesson. Let's assume a feature vector representing a bicycle has three features: number of wheels, mass (kg), and speed (km/h). We can construct a bicycle feature vector as follows: $\\mathbf{x}=[2, 9, 15]$. This seems reasonable, as a bike has 2 wheels, weighs 9kg, and can reach 15km/h. We could use many more features to identify a bicycle, but let's assume this is sufficient to identify bicycles against other concepts. In this case, the number of features for vector $\\mathbf{x}$ is $3$, so we can easily plot this three-dimensional vector in an xyz-coordinate plane. With a sufficiently large number of features, it will be impossible to visualize on a coordinate plane for the time being until algorithms for Dimensionality Reduction are introduced.\n",
    "\n",
    "If we attempt to classify objects based on a single feature, say number of wheels, we can easily form a rule around what can be a bicycle and what cannot, regardless of accuracy, since any 2 wheel object will be a bicycle. This will clearly going to result in false classifications, since a scooter, or motorcycle also satisfy this rule. When we add more features we start to make more accurate bicycle classifications, so our intuition leads to greatly increasing the dimensionality to make near perfect classifications. We will cover in later sections that more features (ie. higher dimensions) are not always better. If you are curious as to why this is the case, read about the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality), which will be covered extensively in future lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Data\n",
    "\n",
    "The goal of a classifier, as mentioned above, is to classify an example of data into a finite set of classifications. Let us load a toy dataset describing attributes of the iris plant species to familiarize ourselves with SciKit Learn's toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the SciKit Learn library datasets\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading to toy iris dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# Description of the dataset\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is very simple - there are 150 examples, each with 4 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shape of the data is the \"matrix\" view\n",
    "# For this set, we are looking at a 150 (examples) x 4 (features) set\n",
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The four features of the plant we use to classify\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The three target classifications we use to classify our features\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 examples in the dataset.\n",
    "# Each row is a feature vector, each column is a feature\n",
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the example above, we have 150 4-dimensional feature vectors (examples), with a one-dimensional target vector with 150 labels (0='setosa', 1='versicolor', 2='virginica')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *k*-Nearest Neighbor (kNN)\n",
    "\n",
    "For the Iris dataset above, we will apply the kNN algorithm to classify species of Iris plants into one of three categories. First, we need some intuition behind what kNN is.\n",
    "\n",
    "kNN is a distance-based algorithm, where a class label is assigned to a novel example based on how close the example is to existing examples. The proverb \"birds of a feather flock together\" is an excellent way to conceptualize this, as kNN functions under the assumption that classes with a certain label will be clustered near eachother.\n",
    "\n",
    "The $k$ in kNN represents the number of \"nearest neighbors\" to our new \"point\" on a plane. For example, if $k=1$, our new data point will share the class label with the closest known point's label. If $k=2$, then the new data point will share the class label with the majority class nearest to it. In general, $k$ should be chosen as odd to avoid tie-breaking scenarios. In the case of $k=2$, the new data point may have 2 closest neighbors belonging to different classes. When there is no majority class label we need a method to determine the true class label. A trivial solution to this is to take the point that is definitively closer and choose that as the class label, but there exists other solutions for choosing tie-breaks depending on your needs.\n",
    "\n",
    "kNN is also used to solve regression problems, and the output won't be very different. We proceed by finding the $k$ nearest points and simply returning the average of those points.\n",
    "\n",
    "Using the Iris dataset collected above, let's plot our 150 examples, but since we are plotting on a two-dimensional plane, we will consider only sepal length vs. petal width, defined as measurement 0 and 3 respectively in our iris.feature_names array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Iris Dataset: Sepal Length vs. Petal Width')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "# Different colors for each class:\n",
    "#   red = setosa\n",
    "#   blue = versicolor\n",
    "#   green = virginica\n",
    "colors = ['red', 'blue', 'green']\n",
    "# now we'll loop over the points in our dataset, and plot them one at a time\n",
    "for i in range(len(iris.data)):\n",
    "    x = iris.data[i][0]\n",
    "    y = iris.data[i][3]\n",
    "    # use the target (which we know is 0, 1, or 2) to select the color for this point\n",
    "    c = colors[iris.target[i]]\n",
    "    # plot the point as a single point in a scatter graph\n",
    "    plt.scatter(x, y, color=c)\n",
    "# now let's add some axis labels; we'll use the names from the dataset\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "# if we want a key, we'll need to make \"handles\" attaching colors to names\n",
    "red = mpatches.Patch(color='red', label='setosa')\n",
    "blue = mpatches.Patch(color='blue', label='versicolor')\n",
    "orange = mpatches.Patch(color='green', label='virginica')\n",
    "plt.legend(handles=[red, blue, orange])\n",
    "plt.title('Iris Dataset: Sepal Length vs. Petal Width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we see a novel iris plant with a sepal length of 5cm and a petal width of 0.25cm, we can reasonably assume that that plant is setosa species, since it falls in the middle of the red 'setosa' cluster (barring information about petal length, and sepal width for the sake of our example).\n",
    "\n",
    "This is the basis for our intuition behind kNN, where new examples are given a label based on how close they are to known neighboring data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Intuition\n",
    "\n",
    "For any given value of $k\\in\\mathbb{Z}$ where $k\\geq1$, read as $k$ is a positive integer, we will find the $k$ nearest existing data points to our new unlabeled data point. For our distance measure, we will be calculating the Euclidean distance between two vectors $\\mathbf{a}=[a_1\\:\\dots\\:a_n]$, and $\\mathbf{b}=[b_1\\:\\dots\\:b_n]$ in the real Euclidean space $\\mathbb{R}^n$ by a function $d:\\mathbb{R}^n,\\mathbb{R}^n\\rightarrow\\mathbb{R}$:\n",
    "\n",
    "\\begin{equation}\n",
    "d(\\mathbf{a},\\mathbf{b})=\\sqrt{(a_1-b_1)^2+\\dots+(a_n-b_n)^2}\n",
    "\\label{eq:euclidean_dist} \\tag{2.3}\n",
    "\\end{equation}\n",
    "\n",
    "Alternatively, we could choose a different distance function, like Manhattan or Hamming distance with different results, but for the sake of simplicity Euclidean distance will suffice. There are often dozens of tunable **hyper-parameters** for a model; in this case, defining the distance function used by our kNN model.\n",
    "\n",
    "Formally, we define kNN as the probability a novel feature vector $\\mathbf{x}$ is classified by label $i$:\n",
    "\n",
    "\\begin{equation}\n",
    "P(y=i|X=\\mathbf{x})=\\frac{1}{k}\\sum_{\\mathbf{j}\\in N}I(y^{(\\mathbf{j})}=i)\n",
    "\\label{eq:prob_knn}\\tag{2.4}\n",
    "\\end{equation}\n",
    "\n",
    "Let's break down \\eqref{eq:prob_knn}, as there are many unknown variables and notations.\n",
    "\n",
    "1. $P(y=i|X=\\mathbf{x})$ is the conditional probability, read as \"The probability of class label $i$ given example $\\mathbf{x}\\in X$\". \n",
    "2. We have already defined $y$ as the set of class labels for all examples, thus $i\\in y$. \n",
    "3. $k$ is the number of nearest neighbors we consider, and is the primary hyper-parameter we tune for this model. As stated above, $k$ must be a positive integer. \n",
    "4. $N$ is the set of $k$ existing examples closest to $\\mathbf{x}$. Hence, $\\mathbf{j}\\in N$ are examples $\\mathbf{j}$ belonging to $N$ that are close to $\\mathbf{x}$. $y^{(\\mathbf{j})}$ is the class label of existing example $\\mathbf{j}$ as defined above.\n",
    "5. $I$ is the indicator function, evaluating to 1 if $y^{(\\mathbf{j})}=i$ is true, and 0 otherwise.\n",
    "\n",
    "We can view the \\eqref{eq:prob_knn} as follows: The probability that an example $\\mathbf{x}$ is classified by label $i$ is equal to the $k$ labels in our dataset, $y$, that are close to $\\mathbf{x}$ (ie. $y^{(\\mathbf{j})}$ that also match classification $i$, divided by the total number of nearest neighbors, $k$.\n",
    "\n",
    "Looking at mathematical notation can be a bit confusing. Over time, if you allow yourself to think through what the definitions mean it becomes much more powerful than the broader definition from the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "Here is a general algorithm for kNN:\n",
    "\n",
    "1. Assume all examples are feature vectors in $\\mathbb{R}^n$ space. (ie. they have real-valued features, since distance can't be calculated on non-real values!)\n",
    "2. For each known example in the dataset, Calculate the distance between this point, and the novel example.\n",
    "3. Record the distance into an array.\n",
    "4. Sort by smallest distance, taking the first *k* distances.\n",
    "5. Get the labels corresponding to the *k* smallest entries.\n",
    "6. Return the most frequently seen of the *k* closest labels as the new class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Action\n",
    "\n",
    "Let's explore kNN using the SciKit Learn model definition of [Nearest Neighbors](https://scikit-learn.org/stable/modules/neighbors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(metric='euclidean', n_neighbors=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a nearest neighbor classifier\n",
    "# Note: Use 'n_neighbors' parameter to control the K in our KNN\n",
    "kNN_classifier_1 = neighbors.KNeighborsClassifier(metric=\"euclidean\", n_neighbors=1)\n",
    "# Fit the data to our classifier, using data and target corresponding to the real data and class labels\n",
    "kNN_classifier_1.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a fitted classifier, we can now test how accurate it can predict novel examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result is the percentage of examples it gets right\n",
    "kNN_classifier_1.score(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think this accuracy score seems a little too good to be true... you would be correct! The model is simply memorizing the training examples, and being fed the exact same examples in the testing phase. We aren't generalizing anything new in this case, just proving that our model is really good at recognizing 150 examples!\n",
    "\n",
    "In order to get a real accuracy score, we need to split our dataset into a train and test set, so when we train our model we can test it's ability to generalize new inputs using the never-before-seen test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (90, 4) , Testing Shape: (60, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "print(\"Training Shape:\", X_train.shape, \", Testing Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6. , 3.4, 4.5, 1.6],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.6, 2.9, 3.6, 1.3]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Cell intentionally left to observe train_test_split characteristics. View data arrays (set X), and corresponding class labels (set y)\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this time to experiment splitting the dataset into separate train and test sets by something other than a 60/40 split, or changing the random_state and observe the first few examples (see how we view data in the section \"Exploration of Data\" above).\n",
    "\n",
    "You should notice that the random_state field is essentially a \"seed\" value for the random number generator that splits our dataset. Regardless of how many times you run the train/test split on the 150 iris data points it will consistently return the same set of pseudo-random data. This is useful for splitting datasets to compare relative performance with other models (or even altered hyper-parameters in the same model). If chose not to set a random_state, then it is possible the data could be split in such a way that the training model would never see an example of a specific class (although highly unlikely), making algorithm comparisons unreliable.\n",
    "\n",
    "Generally, the training split is higher than the testing split because we want more examples to train our model than to evaluate it's accuracy. This will become more obvious as we explore more models, so for now assume a test_size $\\leq0.5$ as a best practice.\n",
    "\n",
    "Now that we have a training set, let's re-fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kNN_classifier_2 = neighbors.KNeighborsClassifier(metric=\"euclidean\", n_neighbors=1)\n",
    "kNN_classifier_2.fit(X_train, y_train)\n",
    "kNN_classifier_2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are fitting our model correctly based on the training data and training labels, and evaluating the model on the test data and test labels. The score function will return an accuracy that is more realistic.\n",
    "\n",
    "Applying kNN is quite simple, and provides a reasonable level of predictive accuracy on low-dimensional, tightly clustered datasets. As we will see in future sections, this algorithm will be out-classed by more powerful predictive algorithms, and is generally not performant on higher dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To practice, and demonstrate your mastery of kNN, you will implement your own kNN classifier. Using the SciKit Learn definition as a scaffolding, generate a kNN classifier method for the iris dataset achieving similar results. Make sure to test for different values of $k$.\n",
    "\n",
    "First, create a Euclidean distance function based on our equation above, $d(\\mathbf{a},\\mathbf{b})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(a, b):\n",
    "    # TODO: a, and b should be arrays. Using the formula defined above find the Euclidean distance between two same-length arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the distance function in a custom kNN algorithm implementation below. Follow the algorithm steps above, and verify the accuracy of your model against the SciKit Learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(train, train_labels, test, test_labels, k):\n",
    "    # TODO: Given a split train/test set, associated labels, and integer k>=1, implement kNN. Use the distance function above to calculate distance between examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
